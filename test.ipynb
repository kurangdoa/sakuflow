{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/04 20:30:49 WARN Utils: Your hostname, Rhyandos-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.178.241 instead (on interface en0)\n",
      "24/06/04 20:30:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/04 20:30:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# test edit\n",
    "# based on https://mybinder.org/v2/gh/projectnessie/nessie-demos/main?filepath=notebooks/nessie-iceberg-demo-nba.ipynb\n",
    "\n",
    "import findspark\n",
    "import os\n",
    "\n",
    "findspark.init(spark_home='/Users/rhyando/spark')\n",
    "os.environ['JAVA_HOME']='/opt/homebrew/Cellar/openjdk@11/11.0.23/libexec/openjdk.jdk/Contents/Home'\n",
    "# os.environ['SPARK_LOCAL_IP']='192.168.49.2'\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import socket\n",
    "\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.projectnessie.nessie-integrations:nessie-spark-extensions-3.3_2.12:0.82.0'\n",
    "\n",
    "conf = pyspark.SparkConf().setAll([\n",
    "    # ip of spark master\n",
    "    # ('spark.driver.host', '0.0.0.0')\n",
    "    # , ('spark.driver.bindAddress', '0.0.0.0')\n",
    "    # ('spark.driver.host', socket.gethostbyname(socket.gethostname()))\n",
    "    #, ('spark.ui.proxyBase', os.environ['JUPYTERHUB_SERVICE_PREFIX'] + 'proxy/4040')\n",
    "    # ('spark.submit.deployMode', 'client')\n",
    "    # , ('spark.driver.port', '7887')\n",
    "    ('spark.master', \"spark://127.0.0.1:7077\")\n",
    "    # , ('spark.jars.packages', 'org.projectnessie.nessie-integrations:nessie-spark-extensions-3.3_2.12:0.82.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-aws:3.3.4')\n",
    "    , (\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\")\n",
    "    # cluster ip\n",
    "    # , ('spark.hadoop.fs.s3a.endpoint', 'http://10.104.71.148:6544')\n",
    "    # , ('spark.hadoop.fs.s3a.endpoint', \"http://minio-service.minio-dev.svc.cluster.local:6544\")\n",
    "    # for minio spark\n",
    "    , ('spark.hadoop.fs.s3a.access.key','minio')\n",
    "    , ('spark.hadoop.fs.s3a.secret.key', 'minio123')\n",
    "    , ('spark.hadoop.fs.s3a.path.style.access', 'true')\n",
    "    , (\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    , (\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    , ('spark.hadoop.fs.s3a.endpoint', \"http://127.0.0.1:6544\")\n",
    "    # for iceberg minio jdbc\n",
    "    #, ('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebesrgSparkSessionExtensions')\n",
    "    , ('spark.sql.catalog.nessie_catalog', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "    , (\"spark.sql.catalog.nessie_catalog.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    , ('spark.sql.defaultCatalog', 'nessie_catalog')\n",
    "    , ('spark.sql.catalog.nessie_catalog.warehouse', 's3a://iceberg/')\n",
    "    # , ('spark.sql.catalog.my_catalog.type', 'jdbc')\n",
    "    # ip of service postgre\n",
    "    , ('spark.sql.catalog.nessie_catalog.uri', 'http://127.0.0.1:6788/api/v1')\n",
    "    # , ('spark.sql.catalog.nessie_catalog.uri', 'http://nessie-service.nessie-dev.svc.cluster.local:6788/api/v1')\n",
    "    , ('spark.sql.catalog.nessie_catalog.ref', 'main')\n",
    "    , (\"spark.sql.catalog.nessie_catalog.authentication.type\", 'NONE')\n",
    "    # , ('spark.sql.catalog.my_catalog.jdbc.user', 'postgres')\n",
    "    # , ('spark.sql.catalog.my_catalog.jdbc.password', 'postgres')\n",
    "    ])\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getenv('SPARK_LOCAL_IP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'127.0.0.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "socket.gethostbyname(socket.gethostname())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------------------+\n",
      "|refType|name|                hash|\n",
      "+-------+----+--------------------+\n",
      "| Branch| dev|377f3500db1793fcf...|\n",
      "| Branch|main|677a4462c3b296f12...|\n",
      "+-------+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"LIST REFERENCES IN nessie_catalog\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/03 21:51:52 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"CREATE TABLE IF NOT EXISTS nessie_catalog.datasaku.salaries\n",
    "            (Season STRING, Team STRING, Salary STRING, Player STRING)\"\"\"\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/04 19:27:40 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"CREATE TABLE IF NOT EXISTS nessie_catalog.datasaku.salaries_test\n",
    "            (Season STRING, Team STRING, Salary STRING, Player STRING)\"\"\"\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------------------+\n",
      "|refType|name|                hash|\n",
      "+-------+----+--------------------+\n",
      "| Branch| dev|377f3500db1793fcf...|\n",
      "+-------+----+--------------------+\n",
      "\n",
      "+-------+----+--------------------+\n",
      "|refType|name|                hash|\n",
      "+-------+----+--------------------+\n",
      "| Branch| dev|377f3500db1793fcf...|\n",
      "+-------+----+--------------------+\n",
      "\n",
      "+---------+-------------+-----------+\n",
      "|namespace|    tableName|isTemporary|\n",
      "+---------+-------------+-----------+\n",
      "| datasaku|     salaries|      false|\n",
      "| datasaku|salaries_test|      false|\n",
      "+---------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"USE REFERENCE dev IN nessie_catalog\").show()\n",
    "spark.sql(\"SHOW REFERENCE IN nessie_catalog\").show()\n",
    "spark.sql(\"SHOW TABLES IN nessie_catalog\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO nessie_catalog.datasaku.salaries VALUES ('1', 'bulls', '50', 'kurdo'), ('2', 'bulls', '51', 'kurdo');\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/04 20:30:55 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+------+\n",
      "|Season|Team|Salary|Player|\n",
      "+------+----+------+------+\n",
      "+------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"select * from nessie_catalog.datasaku.salaries\"\"\"\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasaku'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasaku\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasaku_spark\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasaku'"
     ]
    }
   ],
   "source": [
    "from datasaku import datasaku_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
