{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init(spark_home='/opt/spark')\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-arm64/'\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/local/bin/python3.11'\n",
    "# os.environ['PYSPARK_PYTHON'] = './environment/bin/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/home/datasaku/.venv/bin/python3'\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = './environment/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.9\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\t\t README.md\t   sakuflow.egg-info  util\n",
      "..\t\t dagster.yaml\t   spark-warehouse    workspace.yaml\n",
      ".git\t\t flow\t\t   test.ipynb\n",
      ".gitignore\t pyproject.toml    test_local.ipynb\n",
      ".python-version  requirements.txt  tmpr8phikge\n"
     ]
    }
   ],
   "source": [
    "!ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 19:42:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# test edit\n",
    "# based on https://mybinder.org/v2/gh/projectnessie/nessie-demos/main?filepath=notebooks/nessie-iceberg-demo-nba.ipynb\n",
    "# \n",
    "# to run the spark with custom site_package\n",
    "# zip -r site.zip .\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import socket\n",
    "\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.projectnessie.nessie-integrations:nessie-spark-extensions-3.3_2.12:0.82.0'\n",
    "# os.environ['JAVA_HOME']='/usr/lib/jvm/java-11-openjdk-arm64'\n",
    "conf = pyspark.SparkConf().setAll([\n",
    "    # ip of spark master\n",
    "    # ('spark.driver.bindAddress', '10.244.0.14')\n",
    "    ('spark.driver.host', socket.gethostbyname(socket.gethostname()))\n",
    "    #, ('spark.ui.proxyBase', os.environ['JUPYTERHUB_SERVICE_PREFIX'] + 'proxy/4040')\n",
    "    # ('spark.submit.deployMode', 'client')\n",
    "    # , ('spark.driver.port', '7077')\n",
    "    , ('spark.app.name', 'test')\n",
    "    , ('spark.master', \"spark://spark-master.spark-dev.svc.cluster.local:7077\")\n",
    "    # , ('spark.jars.packages', 'org.projectnessie.nessie-integrations:nessie-spark-extensions-3.3_2.12:0.82.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.hadoop:hadoop-aws:3.3.4')\n",
    "    , (\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions\")\n",
    "    # cluster ip\n",
    "    # , ('spark.hadoop.fs.s3a.endpoint', 'http://10.104.71.148:6544')\n",
    "    , ('spark.hadoop.fs.s3a.endpoint', \"http://minio-service.minio-dev.svc.cluster.local:6544\")\n",
    "    # for minio spark\n",
    "    , ('spark.hadoop.fs.s3a.access.key','minio')\n",
    "    , ('spark.hadoop.fs.s3a.secret.key', 'minio123')\n",
    "    , ('spark.hadoop.fs.s3a.path.style.access', 'true')\n",
    "    , (\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    # for iceberg minio jdbc\n",
    "    #, ('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebesrgSparkSessionExtensions')\n",
    "    , ('spark.sql.catalog.nessie_catalog', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "    , (\"spark.sql.catalog.nessie_catalog.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "    , ('spark.sql.defaultCatalog', 'nessie_catalog')\n",
    "    , ('spark.sql.catalog.nessie_catalog.warehouse', 's3a://iceberg/')\n",
    "    # , ('spark.sql.catalog.my_catalog.type', 'jdbc')\n",
    "    # ip of service postgre\n",
    "    , ('spark.sql.catalog.nessie_catalog.uri', 'http://nessie-service.nessie-dev.svc.cluster.local:6788/api/v1')\n",
    "    , ('spark.sql.catalog.nessie_catalog.ref', 'main')\n",
    "    , (\"spark.sql.catalog.nessie_catalog.authentication.type\", 'NONE')\n",
    "    # , ('spark.sql.catalog.my_catalog.jdbc.user', 'postgres')\n",
    "    # , ('spark.sql.catalog.my_catalog.jdbc.password', 'postgres')\n",
    "    # , ('spark.pyspark.driver.python', './environment/bin/python')\n",
    "    # , ('spark.driver.python', './environment/bin/python')\n",
    "    # , (\"spark.archives\", '/home/datasaku/pyspark_venv.tar.gz#environment') # not working\n",
    "    , (\"spark.submit.pyFiles\", \"/home/datasaku/.venv/lib/python3.11/site-packages/site.zip\")\n",
    "    ])\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Table Manual\n",
    "\n",
    "https://iceberg.apache.org/docs/latest/spark-ddl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------------------+\n",
      "|refType|name|                hash|\n",
      "+-------+----+--------------------+\n",
      "| Branch|main|d24cf9d4e15c69257...|\n",
      "+-------+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"LIST REFERENCES IN nessie_catalog\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[SCHEMA_ALREADY_EXISTS] Cannot create schema `datasaku` because it already exists.\nChoose a different name, drop the existing schema, or add the IF NOT EXISTS clause to tolerate pre-existing schema.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCREATE NAMESPACE nessie_catalog.datasaku\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [SCHEMA_ALREADY_EXISTS] Cannot create schema `datasaku` because it already exists.\nChoose a different name, drop the existing schema, or add the IF NOT EXISTS clause to tolerate pre-existing schema."
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE NAMESPACE nessie_catalog.datasaku\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 19:54:24 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"CREATE TABLE IF NOT EXISTS nessie_catalog.datasaku.salaries\n",
    "            (Season STRING, Team STRING, Salary STRING, Player STRING)\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO nessie_catalog.datasaku.salaries VALUES ('1', 'bulls', '50', 'kurdo'), ('2', 'bulls', '51', 'kurdo');\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+------+\n",
      "|Season| Team|Salary|Player|\n",
      "+------+-----+------+------+\n",
      "|     1|bulls|    50| kurdo|\n",
      "|     2|bulls|    51| kurdo|\n",
      "|     1|bulls|    50| kurdo|\n",
      "|     2|bulls|    51| kurdo|\n",
      "|     1|bulls|    50| kurdo|\n",
      "|     2|bulls|    51| kurdo|\n",
      "|     1|bulls|    50| kurdo|\n",
      "|     2|bulls|    51| kurdo|\n",
      "|     1|bulls|    50| kurdo|\n",
      "|     2|bulls|    51| kurdo|\n",
      "+------+-----+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/08 05:35:58 ERROR TaskSchedulerImpl: Lost executor 0 on 10.244.1.207: worker lost: Not receiving heartbeat for 60 seconds\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from nessie_catalog.datasaku.salaries;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msdf\u001b[49m\u001b[38;5;241m.\u001b[39mwriteTo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnessie_catalog.datasaku.salaries\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mappend()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sdf' is not defined"
     ]
    }
   ],
   "source": [
    "sdf.writeTo(\"nessie_catalog.datasaku.salaries\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.table(\"nessie_catalog.datasaku.salaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_agg = sdf.groupBy('Team').count()\n",
    "sdf_agg.createOrReplaceTempView(\"salaries_agg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| Team|count|\n",
      "+-----+-----+\n",
      "|bulls|    4|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from salaries_agg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table nessie_catalog.datasaku.salaries_agg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_agg.writeTo(\"nessie_catalog.datasaku.salaries_agg\").create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------------------+\n",
      "|refType|name|                hash|\n",
      "+-------+----+--------------------+\n",
      "| Branch|main|2f8f9bc41a8b1b963...|\n",
      "+-------+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW REFERENCE IN nessie_catalog\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+-----------+\n",
      "|namespace|   tableName|isTemporary|\n",
      "+---------+------------+-----------+\n",
      "| datasaku|dagster_test|      false|\n",
      "| datasaku|    salaries|      false|\n",
      "| datasaku|salaries_agg|      false|\n",
      "+---------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN nessie_catalog.datasaku\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| Team|count|\n",
      "+-----+-----+\n",
      "|bulls|    4|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from nessie_catalog.datasaku.salaries_agg\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read From File\n",
    "\n",
    "https://spark.apache.org/docs/2.2.1/sql-programming-guide.html#datasets-and-dataframes\n",
    "https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumnRenamed.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+-------+----------------------+\n",
      "|Year|Industry_aggregation_NZSIOC|Industry_code_NZSIOC|Industry_name_NZSIOC|             Units|Variable_code|       Variable_name|   Variable_category|  Value|Industry_code_ANZSIC06|\n",
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+-------+----------------------+\n",
      "|2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H01|        Total income|Financial perform...|757,504|  ANZSIC06 division...|\n",
      "|2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H04|Sales, government...|Financial perform...|674,890|  ANZSIC06 division...|\n",
      "|2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H05|Interest, dividen...|Financial perform...| 49,593|  ANZSIC06 division...|\n",
      "|2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H07|Non-operating income|Financial perform...| 33,020|  ANZSIC06 division...|\n",
      "|2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H08|   Total expenditure|Financial perform...|654,404|  ANZSIC06 division...|\n",
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+-------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = spark.read.csv('s3a://input/nz.csv', header=True)\n",
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Year|\n",
      "+----+\n",
      "|2021|\n",
      "|2021|\n",
      "|2021|\n",
      "|2021|\n",
      "|2021|\n",
      "+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.select(sdf.Year).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|Year|             Units|\n",
      "+----+------------------+\n",
      "|2021|Dollars (millions)|\n",
      "|2021|Dollars (millions)|\n",
      "|2021|Dollars (millions)|\n",
      "|2021|Dollars (millions)|\n",
      "|2021|Dollars (millions)|\n",
      "+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.select('Year', 'Units').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+-------+----------------------+\n",
      "|Year|Industry_aggregation_NZSIOC|Industry_code_NZSIOC|Industry_name_NZSIOC|             Units|Variable_code|       Variable_name|   Variable_category|  Value|Industry_code_ANZSIC06|\n",
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+-------+----------------------+\n",
      "|2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H01|        Total income|Financial perform...|757,504|  ANZSIC06 division...|\n",
      "|2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H04|Sales, government...|Financial perform...|674,890|  ANZSIC06 division...|\n",
      "|2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H05|Interest, dividen...|Financial perform...| 49,593|  ANZSIC06 division...|\n",
      "|2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H07|Non-operating income|Financial perform...| 33,020|  ANZSIC06 division...|\n",
      "|2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H08|   Total expenditure|Financial perform...|654,404|  ANZSIC06 division...|\n",
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+-------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.filter(sdf.Year == 2021).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+-------+----------------------+\n",
      "|Year|Industry_aggregation_NZSIOC|Industry_code_NZSIOC|Industry_name_NZSIOC|             Units|Variable_code|       Variable_name|   Variable_category|  Value|Industry_code_ANZSIC06|\n",
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+-------+----------------------+\n",
      "|2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H01|        Total income|Financial perform...|757,504|  ANZSIC06 division...|\n",
      "|2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H04|Sales, government...|Financial perform...|674,890|  ANZSIC06 division...|\n",
      "|2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H05|Interest, dividen...|Financial perform...| 49,593|  ANZSIC06 division...|\n",
      "|2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H07|Non-operating income|Financial perform...| 33,020|  ANZSIC06 division...|\n",
      "|2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H08|   Total expenditure|Financial perform...|654,404|  ANZSIC06 division...|\n",
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+-------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as fs\n",
    "\n",
    "sdf.filter(fs.col('Year') == '2021').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+-------+----------------------+\n",
      "|Year Test|Industry_aggregation_NZSIOC|Industry_code_NZSIOC|Industry_name_NZSIOC|             Units|Variable_code|       Variable_name|   Variable_category|  Value|Industry_code_ANZSIC06|\n",
      "+---------+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+-------+----------------------+\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H01|        Total income|Financial perform...|757,504|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H04|Sales, government...|Financial perform...|674,890|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H05|Interest, dividen...|Financial perform...| 49,593|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H07|Non-operating income|Financial perform...| 33,020|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H08|   Total expenditure|Financial perform...|654,404|  ANZSIC06 division...|\n",
      "+---------+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+-------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf1 = sdf.alias('sdf1')\n",
    "sdf2 = sdf.alias('sdf2')\n",
    "sdf2 = sdf2.withColumnRenamed('Year', 'Year Test')\n",
    "sdf2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+---------+----------------------+\n",
      "|Year_Test|Industry_aggregation_NZSIOC|Industry_code_NZSIOC|Industry_name_NZSIOC|             Units|Variable_code|       Variable_name|   Variable_category|    Value|Industry_code_ANZSIC06|\n",
      "+---------+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+---------+----------------------+\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H01|        Total income|Financial perform...|  757,504|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H04|Sales, government...|Financial perform...|  674,890|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H05|Interest, dividen...|Financial perform...|   49,593|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H07|Non-operating income|Financial perform...|   33,020|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H08|   Total expenditure|Financial perform...|  654,404|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H09|Interest and dona...|Financial perform...|   26,138|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H10|      Indirect taxes|Financial perform...|    6,991|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H11|        Depreciation|Financial perform...|   27,801|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H12|Salaries and wage...|Financial perform...|  123,620|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H13|Redundancy and se...|Financial perform...|      275|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H14|Salaries and wage...|Financial perform...|    2,085|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H19|Purchases and oth...|Financial perform...|  452,963|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H20|Non-operating exp...|Financial perform...|   14,806|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H21|      Opening stocks|Financial perform...|   68,896|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H22|      Closing stocks|Financial perform...|   69,127|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H23|Surplus before in...|Financial perform...|  103,330|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H24|        Total assets|  Financial position|2,512,677|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H25|      Current assets|  Financial position|  730,587|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H26|Fixed tangible as...|  Financial position|  591,351|  ANZSIC06 division...|\n",
      "|     2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H29|        Other assets|  Financial position|1,190,739|  ANZSIC06 division...|\n",
      "+---------+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+---------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "change_column = [x.replace(' ','_') for x in sdf2.columns]\n",
    "sdf2 = sdf2.toDF(*change_column)\n",
    "sdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+-------------+--------------------+-------+----------------------+---------+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+------------------+------+----------------------+\n",
      "|Year|Industry_aggregation_NZSIOC|Industry_code_NZSIOC|Industry_name_NZSIOC|             Units|Variable_code|Variable_name|   Variable_category|  Value|Industry_code_ANZSIC06|Year_Test|Industry_aggregation_NZSIOC|Industry_code_NZSIOC|Industry_name_NZSIOC|             Units|Variable_code|       Variable_name| Variable_category| Value|Industry_code_ANZSIC06|\n",
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+-------------+--------------------+-------+----------------------+---------+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+------------------+------+----------------------+\n",
      "|2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H01| Total income|Financial perform...|757,504|  ANZSIC06 division...|     2021|                    Level 3|                ZZ11|Food Product Manu...|Dollars (millions)|          H33|   Other liabilities|Financial position| 8,913|  ANZSIC06 groups C...|\n",
      "|2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H01| Total income|Financial perform...|757,504|  ANZSIC06 division...|     2021|                    Level 3|                ZZ11|Food Product Manu...|Dollars (millions)|          H32| Current liabilities|Financial position|11,493|  ANZSIC06 groups C...|\n",
      "|2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H01| Total income|Financial perform...|757,504|  ANZSIC06 division...|     2021|                    Level 3|                ZZ11|Food Product Manu...|Dollars (millions)|          H31|Shareholders fund...|Financial position|16,678|  ANZSIC06 groups C...|\n",
      "|2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H01| Total income|Financial perform...|757,504|  ANZSIC06 division...|     2021|                    Level 3|                ZZ11|Food Product Manu...|Dollars (millions)|          H30|Total equity and ...|Financial position|37,083|  ANZSIC06 groups C...|\n",
      "|2021|                    Level 1|               99999|      All industries|Dollars (millions)|          H01| Total income|Financial perform...|757,504|  ANZSIC06 division...|     2021|                    Level 3|                ZZ11|Food Product Manu...|Dollars (millions)|          H29|        Other assets|Financial position| 8,637|  ANZSIC06 groups C...|\n",
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+-------------+--------------------+-------+----------------------+---------+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+------------------+------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# important to setup alias again before joining\n",
    "sdf2 = sdf2.alias('sdf2')\n",
    "join = sdf1.join(sdf2, (sdf1.Year == sdf2.Year_Test) & (sdf1.Units == sdf2.Units))\n",
    "join.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|Year_Test|\n",
      "+---------+\n",
      "|     2021|\n",
      "|     2021|\n",
      "|     2021|\n",
      "|     2021|\n",
      "|     2021|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf2.select(sdf2.Year_Test).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|Year_Test|\n",
      "+---------+\n",
      "|     2021|\n",
      "|     2021|\n",
      "|     2021|\n",
      "|     2021|\n",
      "|     2021|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf2.select(fs.col('Year_Test')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year_Test: string (nullable = true)\n",
      " |-- Industry_aggregation_NZSIOC: string (nullable = true)\n",
      " |-- Industry_code_NZSIOC: string (nullable = true)\n",
      " |-- Industry_name_NZSIOC: string (nullable = true)\n",
      " |-- Units: string (nullable = true)\n",
      " |-- Variable_code: string (nullable = true)\n",
      " |-- Variable_name: string (nullable = true)\n",
      " |-- Variable_category: string (nullable = true)\n",
      " |-- Value: string (nullable = true)\n",
      " |-- Industry_code_ANZSIC06: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|Year|Year_Test|\n",
      "+----+---------+\n",
      "|2021|     2021|\n",
      "|2021|     2021|\n",
      "|2021|     2021|\n",
      "|2021|     2021|\n",
      "|2021|     2021|\n",
      "+----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join.select(fs.col('sdf1.Year'), fs.col('sdf2.Year_Test')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Industry_aggregation_NZSIOC: string (nullable = true)\n",
      " |-- Industry_code_NZSIOC: string (nullable = true)\n",
      " |-- Industry_name_NZSIOC: string (nullable = true)\n",
      " |-- Units: string (nullable = true)\n",
      " |-- Variable_code: string (nullable = true)\n",
      " |-- Variable_name: string (nullable = true)\n",
      " |-- Variable_category: string (nullable = true)\n",
      " |-- Value: string (nullable = true)\n",
      " |-- Industry_code_ANZSIC06: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Industry_aggregation_NZSIOC: string (nullable = true)\n",
      " |-- Industry_code_NZSIOC: string (nullable = true)\n",
      " |-- Industry_name_NZSIOC: string (nullable = true)\n",
      " |-- Units: string (nullable = true)\n",
      " |-- Variable_code: string (nullable = true)\n",
      " |-- Variable_name: string (nullable = true)\n",
      " |-- Variable_category: string (nullable = true)\n",
      " |-- Value: string (nullable = true)\n",
      " |-- Industry_code_ANZSIC06: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iceberg\n",
    "\n",
    "https://iceberg.apache.org/docs/latest/spark-queries/#dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2024-06-04 21:22:...|6974540300266715054|               NULL|   append|s3a://iceberg/dat...|{spark.app.id -> ...|\n",
      "|2024-06-06 17:22:...|4394921702574654866|6974540300266715054|   append|s3a://iceberg/dat...|{spark.app.id -> ...|\n",
      "|2024-06-07 05:36:...|3941761197276919739|4394921702574654866|   append|s3a://iceberg/dat...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from nessie_catalog.datasaku.salaries.snapshots\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2024-06-04 21:22:...|6974540300266715054|               NULL|               true|\n",
      "|2024-06-06 17:22:...|4394921702574654866|6974540300266715054|               true|\n",
      "|2024-06-07 05:36:...|3941761197276919739|4394921702574654866|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from nessie_catalog.datasaku.salaries.history\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+-------+------------+------------------+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+------------+-------------+------------+-------------+--------------------+\n",
      "|content|           file_path|file_format|spec_id|record_count|file_size_in_bytes|        column_sizes|        value_counts|   null_value_counts|nan_value_counts|        lower_bounds|        upper_bounds|key_metadata|split_offsets|equality_ids|sort_order_id|    readable_metrics|\n",
      "+-------+--------------------+-----------+-------+------------+------------------+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+------------+-------------+------------+-------------+--------------------+\n",
      "|      0|s3a://iceberg/dat...|    PARQUET|      0|           1|              1089|{1 -> 36, 2 -> 41...|{1 -> 1, 2 -> 1, ...|{1 -> 0, 2 -> 0, ...|              {}|{1 -> [31], 2 -> ...|{1 -> [31], 2 -> ...|        NULL|          [4]|        NULL|            0|{{40, 1, 0, NULL,...|\n",
      "|      0|s3a://iceberg/dat...|    PARQUET|      0|           1|              1090|{1 -> 37, 2 -> 41...|{1 -> 1, 2 -> 1, ...|{1 -> 0, 2 -> 0, ...|              {}|{1 -> [32], 2 -> ...|{1 -> [32], 2 -> ...|        NULL|          [4]|        NULL|            0|{{40, 1, 0, NULL,...|\n",
      "|      0|s3a://iceberg/dat...|    PARQUET|      0|           1|              1089|{1 -> 36, 2 -> 41...|{1 -> 1, 2 -> 1, ...|{1 -> 0, 2 -> 0, ...|              {}|{1 -> [31], 2 -> ...|{1 -> [31], 2 -> ...|        NULL|          [4]|        NULL|            0|{{40, 1, 0, NULL,...|\n",
      "|      0|s3a://iceberg/dat...|    PARQUET|      0|           1|              1090|{1 -> 37, 2 -> 41...|{1 -> 1, 2 -> 1, ...|{1 -> 0, 2 -> 0, ...|              {}|{1 -> [32], 2 -> ...|{1 -> [32], 2 -> ...|        NULL|          [4]|        NULL|            0|{{40, 1, 0, NULL,...|\n",
      "|      0|s3a://iceberg/dat...|    PARQUET|      0|           1|              1089|{1 -> 36, 2 -> 41...|{1 -> 1, 2 -> 1, ...|{1 -> 0, 2 -> 0, ...|              {}|{1 -> [31], 2 -> ...|{1 -> [31], 2 -> ...|        NULL|          [4]|        NULL|            0|{{40, 1, 0, NULL,...|\n",
      "|      0|s3a://iceberg/dat...|    PARQUET|      0|           1|              1090|{1 -> 37, 2 -> 41...|{1 -> 1, 2 -> 1, ...|{1 -> 0, 2 -> 0, ...|              {}|{1 -> [32], 2 -> ...|{1 -> [32], 2 -> ...|        NULL|          [4]|        NULL|            0|{{40, 1, 0, NULL,...|\n",
      "+-------+--------------------+-----------+-------+------------+------------------+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+------------+-------------+------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from nessie_catalog.datasaku.salaries.files\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+------+\n",
      "|Season| Team|Salary|Player|\n",
      "+------+-----+------+------+\n",
      "|     1|bulls|    50| kurdo|\n",
      "|     2|bulls|    51| kurdo|\n",
      "+------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from nessie_catalog.datasaku.salaries VERSION AS OF 6974540300266715054\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+------+\n",
      "|Season| Team|Salary|Player|\n",
      "+------+-----+------+------+\n",
      "|     1|bulls|    50| kurdo|\n",
      "|     1|bulls|    50| kurdo|\n",
      "|     1|bulls|    50| kurdo|\n",
      "|     2|bulls|    51| kurdo|\n",
      "|     2|bulls|    51| kurdo|\n",
      "|     2|bulls|    51| kurdo|\n",
      "+------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('iceberg').load('nessie_catalog.datasaku.salaries').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+------+\n",
      "|Season| Team|Salary|Player|\n",
      "+------+-----+------+------+\n",
      "|     1|bulls|    50| kurdo|\n",
      "|     2|bulls|    51| kurdo|\n",
      "+------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('snapshot-id', 6974540300266715054).format('iceberg').load('nessie_catalog.datasaku.salaries').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///data/datasaku/datasaku\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: boto3==1.26.47 in /home/datasaku/.venv/lib/python3.11/site-packages (from datasaku==0.0.8) (1.26.47)\n",
      "Requirement already satisfied: Fiona==1.8.22 in /home/datasaku/.venv/lib/python3.11/site-packages (from datasaku==0.0.8) (1.8.22)\n",
      "Requirement already satisfied: fuzzywuzzy==0.18.0 in /home/datasaku/.venv/lib/python3.11/site-packages (from datasaku==0.0.8) (0.18.0)\n",
      "Requirement already satisfied: geopandas==0.12.2 in /home/datasaku/.venv/lib/python3.11/site-packages (from datasaku==0.0.8) (0.12.2)\n",
      "Requirement already satisfied: google-api-python-client==2.72.0 in /home/datasaku/.venv/lib/python3.11/site-packages (from datasaku==0.0.8) (2.72.0)\n",
      "Requirement already satisfied: google-auth-oauthlib==0.8.0 in /home/datasaku/.venv/lib/python3.11/site-packages (from datasaku==0.0.8) (0.8.0)\n",
      "Requirement already satisfied: h3==3.7.6 in /home/datasaku/.venv/lib/python3.11/site-packages (from datasaku==0.0.8) (3.7.6)\n",
      "Requirement already satisfied: importlib-resources==5.10.2 in /home/datasaku/.venv/lib/python3.11/site-packages (from datasaku==0.0.8) (5.10.2)\n",
      "Requirement already satisfied: matplotlib==3.6.2 in /home/datasaku/.venv/lib/python3.11/site-packages (from datasaku==0.0.8) (3.6.2)\n",
      "Requirement already satisfied: minio==7.1.14 in /home/datasaku/.venv/lib/python3.11/site-packages (from datasaku==0.0.8) (7.1.14)\n",
      "Requirement already satisfied: pandas==1.5.2 in /home/datasaku/.venv/lib/python3.11/site-packages (from datasaku==0.0.8) (1.5.2)\n",
      "Requirement already satisfied: plotly==5.11.0 in /home/datasaku/.venv/lib/python3.11/site-packages (from datasaku==0.0.8) (5.11.0)\n",
      "Requirement already satisfied: protobuf==4.23.0 in /home/datasaku/.venv/lib/python3.11/site-packages (from datasaku==0.0.8) (4.23.0)\n",
      "Requirement already satisfied: PyHive==0.6.5 in /home/datasaku/.venv/lib/python3.11/site-packages (from datasaku==0.0.8) (0.6.5)\n",
      "Requirement already satisfied: Requests==2.30.0 in /home/datasaku/.venv/lib/python3.11/site-packages (from datasaku==0.0.8) (2.30.0)\n",
      "Requirement already satisfied: seaborn==0.12.2 in /home/datasaku/.venv/lib/python3.11/site-packages (from datasaku==0.0.8) (0.12.2)\n",
      "Requirement already satisfied: snowflake==0.0.3 in /home/datasaku/.venv/lib/python3.11/site-packages (from datasaku==0.0.8) (0.0.3)\n",
      "Requirement already satisfied: SQLAlchemy==1.4.46 in /home/datasaku/.venv/lib/python3.11/site-packages (from datasaku==0.0.8) (1.4.46)\n",
      "Requirement already satisfied: urllib3==1.26.15 in /home/datasaku/.venv/lib/python3.11/site-packages (from datasaku==0.0.8) (1.26.15)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.47 in /home/datasaku/.venv/lib/python3.11/site-packages (from boto3==1.26.47->datasaku==0.0.8) (1.29.165)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/datasaku/.venv/lib/python3.11/site-packages (from boto3==1.26.47->datasaku==0.0.8) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /home/datasaku/.venv/lib/python3.11/site-packages (from boto3==1.26.47->datasaku==0.0.8) (0.6.2)\n",
      "Requirement already satisfied: attrs>=17 in /home/datasaku/.venv/lib/python3.11/site-packages (from Fiona==1.8.22->datasaku==0.0.8) (23.2.0)\n",
      "Requirement already satisfied: certifi in /home/datasaku/.venv/lib/python3.11/site-packages (from Fiona==1.8.22->datasaku==0.0.8) (2024.6.2)\n",
      "Requirement already satisfied: click>=4.0 in /home/datasaku/.venv/lib/python3.11/site-packages (from Fiona==1.8.22->datasaku==0.0.8) (8.1.7)\n",
      "Requirement already satisfied: cligj>=0.5 in /home/datasaku/.venv/lib/python3.11/site-packages (from Fiona==1.8.22->datasaku==0.0.8) (0.7.2)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /home/datasaku/.venv/lib/python3.11/site-packages (from Fiona==1.8.22->datasaku==0.0.8) (1.1.1)\n",
      "Requirement already satisfied: six>=1.7 in /home/datasaku/.venv/lib/python3.11/site-packages (from Fiona==1.8.22->datasaku==0.0.8) (1.16.0)\n",
      "Requirement already satisfied: munch in /home/datasaku/.venv/lib/python3.11/site-packages (from Fiona==1.8.22->datasaku==0.0.8) (4.0.0)\n",
      "Requirement already satisfied: setuptools in /home/datasaku/.venv/lib/python3.11/site-packages (from Fiona==1.8.22->datasaku==0.0.8) (65.5.0)\n",
      "Requirement already satisfied: shapely>=1.7 in /home/datasaku/.venv/lib/python3.11/site-packages (from geopandas==0.12.2->datasaku==0.0.8) (2.0.4)\n",
      "Requirement already satisfied: pyproj>=2.6.1.post1 in /home/datasaku/.venv/lib/python3.11/site-packages (from geopandas==0.12.2->datasaku==0.0.8) (3.6.1)\n",
      "Requirement already satisfied: packaging in /home/datasaku/.venv/lib/python3.11/site-packages (from geopandas==0.12.2->datasaku==0.0.8) (24.1)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /home/datasaku/.venv/lib/python3.11/site-packages (from google-api-python-client==2.72.0->datasaku==0.0.8) (0.22.0)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /home/datasaku/.venv/lib/python3.11/site-packages (from google-api-python-client==2.72.0->datasaku==0.0.8) (2.30.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /home/datasaku/.venv/lib/python3.11/site-packages (from google-api-python-client==2.72.0->datasaku==0.0.8) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /home/datasaku/.venv/lib/python3.11/site-packages (from google-api-python-client==2.72.0->datasaku==0.0.8) (2.19.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /home/datasaku/.venv/lib/python3.11/site-packages (from google-api-python-client==2.72.0->datasaku==0.0.8) (4.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/datasaku/.venv/lib/python3.11/site-packages (from google-auth-oauthlib==0.8.0->datasaku==0.0.8) (2.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/datasaku/.venv/lib/python3.11/site-packages (from matplotlib==3.6.2->datasaku==0.0.8) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/datasaku/.venv/lib/python3.11/site-packages (from matplotlib==3.6.2->datasaku==0.0.8) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/datasaku/.venv/lib/python3.11/site-packages (from matplotlib==3.6.2->datasaku==0.0.8) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/datasaku/.venv/lib/python3.11/site-packages (from matplotlib==3.6.2->datasaku==0.0.8) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.19 in /home/datasaku/.venv/lib/python3.11/site-packages (from matplotlib==3.6.2->datasaku==0.0.8) (2.0.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/datasaku/.venv/lib/python3.11/site-packages (from matplotlib==3.6.2->datasaku==0.0.8) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/datasaku/.venv/lib/python3.11/site-packages (from matplotlib==3.6.2->datasaku==0.0.8) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/datasaku/.venv/lib/python3.11/site-packages (from matplotlib==3.6.2->datasaku==0.0.8) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/datasaku/.venv/lib/python3.11/site-packages (from pandas==1.5.2->datasaku==0.0.8) (2024.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/datasaku/.venv/lib/python3.11/site-packages (from plotly==5.11.0->datasaku==0.0.8) (8.4.1)\n",
      "Requirement already satisfied: future in /home/datasaku/.venv/lib/python3.11/site-packages (from PyHive==0.6.5->datasaku==0.0.8) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/datasaku/.venv/lib/python3.11/site-packages (from Requests==2.30.0->datasaku==0.0.8) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/datasaku/.venv/lib/python3.11/site-packages (from Requests==2.30.0->datasaku==0.0.8) (3.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/datasaku/.venv/lib/python3.11/site-packages (from SQLAlchemy==1.4.46->datasaku==0.0.8) (3.0.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/datasaku/.venv/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client==2.72.0->datasaku==0.0.8) (1.63.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/datasaku/.venv/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client==2.72.0->datasaku==0.0.8) (1.23.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/datasaku/.venv/lib/python3.11/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client==2.72.0->datasaku==0.0.8) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/datasaku/.venv/lib/python3.11/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client==2.72.0->datasaku==0.0.8) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/datasaku/.venv/lib/python3.11/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client==2.72.0->datasaku==0.0.8) (4.9)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/datasaku/.venv/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib==0.8.0->datasaku==0.0.8) (3.2.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/datasaku/.venv/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.19.0->google-api-python-client==2.72.0->datasaku==0.0.8) (0.6.0)\n",
      "Building wheels for collected packages: datasaku\n",
      "  Building editable for datasaku (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for datasaku: filename=datasaku-0.0.8-0.editable-py3-none-any.whl size=3539 sha256=2ab5ea9fe23f9f678f8880fca43c8f4d4693db2e128e6e5d8ebc6f53129ae0d4\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1qvst43u/wheels/7d/b0/f7/718884e02c821debbabd25201967bfb75892ce674d5f4eb086\n",
      "Successfully built datasaku\n",
      "Installing collected packages: datasaku\n",
      "  Attempting uninstall: datasaku\n",
      "    Found existing installation: datasaku 0.0.8\n",
      "    Uninstalling datasaku-0.0.8:\n",
      "      Successfully uninstalled datasaku-0.0.8\n",
      "Successfully installed datasaku-0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install -e /data/datasaku/datasaku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYSPARK_PYTHON=/home/datasaku/.venv/bin/python3\n",
    "!export SPARK_PYTHONPATH=/home/datasaku/.venv/bin/python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.9\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasaku import datasaku_spark\n",
    "Spark = datasaku_spark.DatasakuSparkNessieMinioIceberg(\n",
    "    spark_home = '/opt/spark'\n",
    "    , minio_username = 'minio'\n",
    "    , minio_password = 'minio123'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------------------+\n",
      "|refType|name|                hash|\n",
      "+-------+----+--------------------+\n",
      "| Branch| dev|f501c935282dce964...|\n",
      "| Branch|main|e6177d6f9a5659eed...|\n",
      "+-------+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with Spark.spark_session_context() as spark_session:\n",
    "    spark_session.sql(\"LIST REFERENCES IN nessie_catalog\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/20 17:53:22 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+------+\n",
      "|Season| Team|Salary|Player|\n",
      "+------+-----+------+------+\n",
      "|     1|bulls|    50| kurdo|\n",
      "|     2|bulls|    51| kurdo|\n",
      "|     1|bulls|    50| kurdo|\n",
      "|     1|bulls|    50| kurdo|\n",
      "|     2|bulls|    51| kurdo|\n",
      "|     2|bulls|    51| kurdo|\n",
      "+------+-----+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = Spark.spark_session()\n",
    "spark.sql(\"select * from nessie_catalog.datasaku.salaries\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with Spark.spark_session_context() as spark_session:\n",
    "#     spark_session.sql(\"DROP TABLE nessie_catalog.datasaku.salaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with Spark.spark_session_context() as spark_session:\n",
    "#     spark_session.sql(\"DROP NAMESPACE nessie_catalog.datasaku\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[SCHEMA_ALREADY_EXISTS] Cannot create schema `datasaku` because it already exists.\nChoose a different name, drop the existing schema, or add the IF NOT EXISTS clause to tolerate pre-existing schema.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCREATE NAMESPACE nessie_catalog.datasaku\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [SCHEMA_ALREADY_EXISTS] Cannot create schema `datasaku` because it already exists.\nChoose a different name, drop the existing schema, or add the IF NOT EXISTS clause to tolerate pre-existing schema."
     ]
    }
   ],
   "source": [
    "spark_session.sql(\"CREATE NAMESPACE nessie_catalog.datasaku\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_session.sql(\n",
    "    \"\"\"CREATE TABLE IF NOT EXISTS nessie_catalog.datasaku.salaries\n",
    "            (Season STRING, Team STRING, Salary STRING, Player STRING)\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------------------+\n",
      "|refType|name|                hash|\n",
      "+-------+----+--------------------+\n",
      "| Branch| dev|f501c935282dce964...|\n",
      "| Branch|main|9833effe4eeee1416...|\n",
      "+-------+----+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "with Spark.spark_session_context() as spark_session:\n",
    "    spark_session.sql(\"LIST REFERENCES IN nessie_catalog\").show()\n",
    "    spark_session.sql(\"INSERT INTO nessie_catalog.datasaku.salaries VALUES ('1', 'bulls', '50', 'kurdo'), ('2', 'bulls', '51', 'kurdo');\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+------+\n",
      "|Season| Team|Salary|Player|\n",
      "+------+-----+------+------+\n",
      "|     1|bulls|    50| kurdo|\n",
      "|     2|bulls|    51| kurdo|\n",
      "|     1|bulls|    50| kurdo|\n",
      "|     2|bulls|    51| kurdo|\n",
      "|     1|bulls|    50| kurdo|\n",
      "|     2|bulls|    51| kurdo|\n",
      "+------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with Spark.spark_session_context() as spark_session:\n",
    "    spark_session.sql(\"select * from nessie_catalog.datasaku.salaries\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Season: string, Team: string, Salary: string, Player: string]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasaku import datasaku_spark\n",
    "Spark = datasaku_spark.DatasakuSparkNessieMinioIceberg(\n",
    "    spark_home = '/opt/spark'\n",
    "    , minio_username = 'minio'\n",
    "    , minio_password = 'minio123'\n",
    ")\n",
    "spark = Spark.spark_session()\n",
    "sdf = spark.read.table(\"nessie_catalog.datasaku.salaries\")\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as fs\n",
    "sdf = Spark.standarized_column_name(sdf)\n",
    "sdf = Spark.trim_all_column(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+------+\n",
      "|season| team|salary|player|\n",
      "+------+-----+------+------+\n",
      "|     1|bulls|    50| kurdo|\n",
      "|     1|bulls|    50| kurdo|\n",
      "|     2|bulls|    51| kurdo|\n",
      "|     2|bulls|    51| kurdo|\n",
      "+------+-----+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|     made_current_at|        snapshot_id|          parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "|2024-06-15 18:47:...|7754386482845123890|               NULL|               true|\n",
      "|2024-06-15 18:49:...|8095789292677777677|7754386482845123890|               true|\n",
      "|2024-06-15 19:34:...|3138826593629674888|8095789292677777677|               true|\n",
      "+--------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = Spark.spark_session()\n",
    "spark.sql('select * from nessie_catalog.datasaku.salaries.history').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with Spark.spark_session_context() as spark_session:\n",
    "    spark_session.sql(\"delete from nessie_catalog.datasaku.salaries\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-------+------------+------------------+------------------------------------+--------------------------------+--------------------------------+----------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+------------+-------------+------------+-------------+------------------------------------------------------------------------------------------------------------------+\n",
      "|content|file_path                                                                                                                                         |file_format|spec_id|record_count|file_size_in_bytes|column_sizes                        |value_counts                    |null_value_counts               |nan_value_counts|lower_bounds                                                           |upper_bounds                                                           |key_metadata|split_offsets|equality_ids|sort_order_id|readable_metrics                                                                                                  |\n",
      "+-------+--------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-------+------------+------------------+------------------------------------+--------------------------------+--------------------------------+----------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+------------+-------------+------------+-------------+------------------------------------------------------------------------------------------------------------------+\n",
      "|0      |s3a://iceberg/datasaku/salaries_537b8d96-1584-4708-82ac-7a4252f73b9e/data/00000-0-a1ad1117-b5c3-4cea-a88d-50b5368620f3-0-00001.parquet            |PARQUET    |0      |1           |1089              |{1 -> 36, 2 -> 41, 3 -> 38, 4 -> 40}|{1 -> 1, 2 -> 1, 3 -> 1, 4 -> 1}|{1 -> 0, 2 -> 0, 3 -> 0, 4 -> 0}|{}              |{1 -> [31], 2 -> [62 75 6C 6C 73], 3 -> [35 30], 4 -> [6B 75 72 64 6F]}|{1 -> [31], 2 -> [62 75 6C 6C 73], 3 -> [35 30], 4 -> [6B 75 72 64 6F]}|NULL        |[4]          |NULL        |0            |{{40, 1, 0, NULL, kurdo, kurdo}, {38, 1, 0, NULL, 50, 50}, {36, 1, 0, NULL, 1, 1}, {41, 1, 0, NULL, bulls, bulls}}|\n",
      "|0      |s3a://iceberg/datasaku/salaries_537b8d96-1584-4708-82ac-7a4252f73b9e/data/00001-1-a1ad1117-b5c3-4cea-a88d-50b5368620f3-0-00001.parquet            |PARQUET    |0      |1           |1090              |{1 -> 37, 2 -> 41, 3 -> 38, 4 -> 40}|{1 -> 1, 2 -> 1, 3 -> 1, 4 -> 1}|{1 -> 0, 2 -> 0, 3 -> 0, 4 -> 0}|{}              |{1 -> [32], 2 -> [62 75 6C 6C 73], 3 -> [35 31], 4 -> [6B 75 72 64 6F]}|{1 -> [32], 2 -> [62 75 6C 6C 73], 3 -> [35 31], 4 -> [6B 75 72 64 6F]}|NULL        |[4]          |NULL        |0            |{{40, 1, 0, NULL, kurdo, kurdo}, {38, 1, 0, NULL, 51, 51}, {37, 1, 0, NULL, 2, 2}, {41, 1, 0, NULL, bulls, bulls}}|\n",
      "|0      |s3a://iceberg/datasaku/salaries_537b8d96-1584-4708-82ac-7a4252f73b9e/data/20240619_070528_00002_wm4ub-9174a718-acfe-447c-83d3-9861c93e1dfe.parquet|PARQUET    |0      |2           |682               |{1 -> 46, 2 -> 73, 3 -> 48, 4 -> 73}|{1 -> 2, 2 -> 2, 3 -> 2, 4 -> 2}|{1 -> 0, 2 -> 0, 3 -> 0, 4 -> 0}|{}              |{1 -> [31], 2 -> [62 75 6C 6C 73], 3 -> [35 30], 4 -> [6B 75 72 64 6F]}|{1 -> [32], 2 -> [62 75 6C 6C 73], 3 -> [35 31], 4 -> [6B 75 72 64 6F]}|NULL        |NULL         |NULL        |0            |{{73, 2, 0, NULL, kurdo, kurdo}, {48, 2, 0, NULL, 50, 51}, {46, 2, 0, NULL, 1, 2}, {73, 2, 0, NULL, bulls, bulls}}|\n",
      "|0      |s3a://iceberg/datasaku/salaries_537b8d96-1584-4708-82ac-7a4252f73b9e/data/00000-0-15507aff-6e7e-45d8-a079-46e29c1d93e7-0-00001.parquet            |PARQUET    |0      |1           |1089              |{1 -> 36, 2 -> 41, 3 -> 38, 4 -> 40}|{1 -> 1, 2 -> 1, 3 -> 1, 4 -> 1}|{1 -> 0, 2 -> 0, 3 -> 0, 4 -> 0}|{}              |{1 -> [31], 2 -> [62 75 6C 6C 73], 3 -> [35 30], 4 -> [6B 75 72 64 6F]}|{1 -> [31], 2 -> [62 75 6C 6C 73], 3 -> [35 30], 4 -> [6B 75 72 64 6F]}|NULL        |[4]          |NULL        |0            |{{40, 1, 0, NULL, kurdo, kurdo}, {38, 1, 0, NULL, 50, 50}, {36, 1, 0, NULL, 1, 1}, {41, 1, 0, NULL, bulls, bulls}}|\n",
      "|0      |s3a://iceberg/datasaku/salaries_537b8d96-1584-4708-82ac-7a4252f73b9e/data/00001-1-15507aff-6e7e-45d8-a079-46e29c1d93e7-0-00001.parquet            |PARQUET    |0      |1           |1090              |{1 -> 37, 2 -> 41, 3 -> 38, 4 -> 40}|{1 -> 1, 2 -> 1, 3 -> 1, 4 -> 1}|{1 -> 0, 2 -> 0, 3 -> 0, 4 -> 0}|{}              |{1 -> [32], 2 -> [62 75 6C 6C 73], 3 -> [35 31], 4 -> [6B 75 72 64 6F]}|{1 -> [32], 2 -> [62 75 6C 6C 73], 3 -> [35 31], 4 -> [6B 75 72 64 6F]}|NULL        |[4]          |NULL        |0            |{{40, 1, 0, NULL, kurdo, kurdo}, {38, 1, 0, NULL, 51, 51}, {37, 1, 0, NULL, 2, 2}, {41, 1, 0, NULL, bulls, bulls}}|\n",
      "+-------+--------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-------+------------+------------------+------------------------------------+--------------------------------+--------------------------------+----------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+------------+-------------+------------+-------------+------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with Spark.spark_session_context() as spark_session:\n",
    "    spark_session.sql(\"select * from nessie_catalog.datasaku.salaries.files\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+------+\n",
      "|Season| Team|Salary|Player|\n",
      "+------+-----+------+------+\n",
      "|     1|bulls|    50| kurdo|\n",
      "|     2|bulls|    51| kurdo|\n",
      "+------+-----+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "with Spark.spark_session_context() as spark_session:\n",
    "    spark_session.sql(\"select * from nessie_catalog.datasaku.salaries\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o200.sql.\n: org.projectnessie.client.rest.NessieInternalServerException: Internal Server Error (HTTP/500): java.lang.RuntimeException: Unhandled SQL exception, caused by org.postgresql.util.PSQLException: Connection to datasaku-postgres-postgresql-ha-pgpool.psql-dev.svc.cluster.local:5433 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections., caused by java.net.ConnectException: Connection refused\n\tat org.projectnessie.client.rest.ResponseCheckFilter.checkResponse(ResponseCheckFilter.java:70)\n\tat org.projectnessie.client.rest.NessieHttpResponseFilter.filter(NessieHttpResponseFilter.java:29)\n\tat org.projectnessie.client.http.impl.jdk11.JavaRequest.lambda$executeRequest$1(JavaRequest.java:143)\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1541)\n\tat java.base/java.util.Collections$UnmodifiableCollection.forEach(Collections.java:1085)\n\tat org.projectnessie.client.http.impl.jdk11.JavaRequest.executeRequest(JavaRequest.java:143)\n\tat org.projectnessie.client.http.HttpRequest.get(HttpRequest.java:106)\n\tat org.projectnessie.client.rest.v1.RestV1TreeClient.getReferenceByName(RestV1TreeClient.java:83)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.projectnessie.client.rest.v1.RestV1Client$ExceptionRewriter.invoke(RestV1Client.java:84)\n\tat com.sun.proxy.$Proxy31.getReferenceByName(Unknown Source)\n\tat org.projectnessie.client.rest.v1.HttpGetReference.get(HttpGetReference.java:34)\n\tat org.apache.iceberg.nessie.UpdateableReference.refresh(UpdateableReference.java:46)\n\tat org.apache.iceberg.nessie.NessieIcebergClient.refresh(NessieIcebergClient.java:110)\n\tat org.apache.iceberg.nessie.NessieTableOperations.doRefresh(NessieTableOperations.java:71)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:97)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:80)\n\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:49)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:166)\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:843)\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:170)\n\tat org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:164)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:42)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"CREATE TABLE IF NOT EXISTS nessie_catalog.datasaku.salaries_test\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43;03m            (Season STRING, Team STRING, Salary STRING, Player STRING)\"\"\"\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o200.sql.\n: org.projectnessie.client.rest.NessieInternalServerException: Internal Server Error (HTTP/500): java.lang.RuntimeException: Unhandled SQL exception, caused by org.postgresql.util.PSQLException: Connection to datasaku-postgres-postgresql-ha-pgpool.psql-dev.svc.cluster.local:5433 refused. Check that the hostname and port are correct and that the postmaster is accepting TCP/IP connections., caused by java.net.ConnectException: Connection refused\n\tat org.projectnessie.client.rest.ResponseCheckFilter.checkResponse(ResponseCheckFilter.java:70)\n\tat org.projectnessie.client.rest.NessieHttpResponseFilter.filter(NessieHttpResponseFilter.java:29)\n\tat org.projectnessie.client.http.impl.jdk11.JavaRequest.lambda$executeRequest$1(JavaRequest.java:143)\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1541)\n\tat java.base/java.util.Collections$UnmodifiableCollection.forEach(Collections.java:1085)\n\tat org.projectnessie.client.http.impl.jdk11.JavaRequest.executeRequest(JavaRequest.java:143)\n\tat org.projectnessie.client.http.HttpRequest.get(HttpRequest.java:106)\n\tat org.projectnessie.client.rest.v1.RestV1TreeClient.getReferenceByName(RestV1TreeClient.java:83)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.projectnessie.client.rest.v1.RestV1Client$ExceptionRewriter.invoke(RestV1Client.java:84)\n\tat com.sun.proxy.$Proxy31.getReferenceByName(Unknown Source)\n\tat org.projectnessie.client.rest.v1.HttpGetReference.get(HttpGetReference.java:34)\n\tat org.apache.iceberg.nessie.UpdateableReference.refresh(UpdateableReference.java:46)\n\tat org.apache.iceberg.nessie.NessieIcebergClient.refresh(NessieIcebergClient.java:110)\n\tat org.apache.iceberg.nessie.NessieTableOperations.doRefresh(NessieTableOperations.java:71)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.refresh(BaseMetastoreTableOperations.java:97)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.current(BaseMetastoreTableOperations.java:80)\n\tat org.apache.iceberg.BaseMetastoreCatalog.loadTable(BaseMetastoreCatalog.java:49)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n\tat java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n\tat org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n\tat org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:166)\n\tat org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:843)\n\tat org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:170)\n\tat org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:164)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:42)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "spark_session.sql(\n",
    "    \"\"\"CREATE TABLE IF NOT EXISTS nessie_catalog.datasaku.salaries_test\n",
    "            (Season STRING, Team STRING, Salary STRING, Player STRING)\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Spark.spark_session_context() as spark_session:\n",
    "    spark_session.sql(\"LIST REFERENCES IN nessie_catalog\").show()\n",
    "    spark_session.sql(\"INSERT INTO nessie_catalog.datasaku.salaries_test VALUES ('1', 'bulls', '50', 'kurdo'), ('2', 'bulls', '51', 'kurdo');\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyspark to iceberg with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///data/datasaku/datasaku\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting boto3==1.26.47 (from datasaku==0.0.8)\n",
      "  Downloading boto3-1.26.47-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting Fiona==1.8.22 (from datasaku==0.0.8)\n",
      "  Downloading Fiona-1.8.22.tar.gz (1.4 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fuzzywuzzy==0.18.0 (from datasaku==0.0.8)\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting geopandas==0.12.2 (from datasaku==0.0.8)\n",
      "  Downloading geopandas-0.12.2-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting google-api-python-client==2.72.0 (from datasaku==0.0.8)\n",
      "  Downloading google_api_python_client-2.72.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting google-auth-oauthlib==0.8.0 (from datasaku==0.0.8)\n",
      "  Downloading google_auth_oauthlib-0.8.0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting h3==3.7.6 (from datasaku==0.0.8)\n",
      "  Downloading h3-3.7.6-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (4.8 kB)\n",
      "Collecting importlib-resources==5.10.2 (from datasaku==0.0.8)\n",
      "  Downloading importlib_resources-5.10.2-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting matplotlib==3.6.2 (from datasaku==0.0.8)\n",
      "  Downloading matplotlib-3.6.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.8 kB)\n",
      "Collecting minio==7.1.14 (from datasaku==0.0.8)\n",
      "  Downloading minio-7.1.14-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pandas==1.5.2 (from datasaku==0.0.8)\n",
      "  Downloading pandas-1.5.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (11 kB)\n",
      "Collecting plotly==5.11.0 (from datasaku==0.0.8)\n",
      "  Downloading plotly-5.11.0-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting protobuf==4.23.0 (from datasaku==0.0.8)\n",
      "  Downloading protobuf-4.23.0-cp37-abi3-manylinux2014_aarch64.whl.metadata (540 bytes)\n",
      "Collecting PyHive==0.6.5 (from datasaku==0.0.8)\n",
      "  Downloading PyHive-0.6.5.tar.gz (44 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting Requests==2.30.0 (from datasaku==0.0.8)\n",
      "  Downloading requests-2.30.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting seaborn==0.12.2 (from datasaku==0.0.8)\n",
      "  Downloading seaborn-0.12.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting snowflake==0.0.3 (from datasaku==0.0.8)\n",
      "  Downloading snowflake-0.0.3.tar.gz (2.1 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting SQLAlchemy==1.4.46 (from datasaku==0.0.8)\n",
      "  Downloading SQLAlchemy-1.4.46-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (10 kB)\n",
      "Collecting urllib3==1.26.15 (from datasaku==0.0.8)\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyfarmhash==0.3.2 in /home/datasaku/.venv/lib/python3.11/site-packages (from datasaku==0.0.8) (0.3.2)\n",
      "Collecting botocore<1.30.0,>=1.29.47 (from boto3==1.26.47->datasaku==0.0.8)\n",
      "  Downloading botocore-1.29.165-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3==1.26.47->datasaku==0.0.8)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3==1.26.47->datasaku==0.0.8)\n",
      "  Downloading s3transfer-0.6.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting attrs>=17 (from Fiona==1.8.22->datasaku==0.0.8)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting certifi (from Fiona==1.8.22->datasaku==0.0.8)\n",
      "  Downloading certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting click>=4.0 (from Fiona==1.8.22->datasaku==0.0.8)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting cligj>=0.5 (from Fiona==1.8.22->datasaku==0.0.8)\n",
      "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting click-plugins>=1.0 (from Fiona==1.8.22->datasaku==0.0.8)\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: six>=1.7 in /home/datasaku/.venv/lib/python3.11/site-packages (from Fiona==1.8.22->datasaku==0.0.8) (1.16.0)\n",
      "Collecting munch (from Fiona==1.8.22->datasaku==0.0.8)\n",
      "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: setuptools in /home/datasaku/.venv/lib/python3.11/site-packages (from Fiona==1.8.22->datasaku==0.0.8) (65.5.0)\n",
      "Collecting shapely>=1.7 (from geopandas==0.12.2->datasaku==0.0.8)\n",
      "  Downloading shapely-2.0.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (7.0 kB)\n",
      "Collecting pyproj>=2.6.1.post1 (from geopandas==0.12.2->datasaku==0.0.8)\n",
      "  Downloading pyproj-3.6.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: packaging in /home/datasaku/.venv/lib/python3.11/site-packages (from geopandas==0.12.2->datasaku==0.0.8) (24.1)\n",
      "Collecting httplib2<1dev,>=0.15.0 (from google-api-python-client==2.72.0->datasaku==0.0.8)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth<3.0.0dev,>=1.19.0 (from google-api-python-client==2.72.0->datasaku==0.0.8)\n",
      "  Downloading google_auth-2.30.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-httplib2>=0.1.0 (from google-api-python-client==2.72.0->datasaku==0.0.8)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 (from google-api-python-client==2.72.0->datasaku==0.0.8)\n",
      "  Downloading google_api_core-2.19.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client==2.72.0->datasaku==0.0.8)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib==0.8.0->datasaku==0.0.8)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib==3.6.2->datasaku==0.0.8)\n",
      "  Downloading contourpy-1.2.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib==3.6.2->datasaku==0.0.8)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib==3.6.2->datasaku==0.0.8)\n",
      "  Downloading fonttools-4.53.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (162 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m162.2/162.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib==3.6.2->datasaku==0.0.8)\n",
      "  Downloading kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.19 in /home/datasaku/.venv/lib/python3.11/site-packages (from matplotlib==3.6.2->datasaku==0.0.8) (1.26.4)\n",
      "Collecting pillow>=6.2.0 (from matplotlib==3.6.2->datasaku==0.0.8)\n",
      "  Downloading pillow-10.3.0-cp311-cp311-manylinux_2_28_aarch64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.2.1 (from matplotlib==3.6.2->datasaku==0.0.8)\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/datasaku/.venv/lib/python3.11/site-packages (from matplotlib==3.6.2->datasaku==0.0.8) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas==1.5.2->datasaku==0.0.8)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tenacity>=6.2.0 (from plotly==5.11.0->datasaku==0.0.8)\n",
      "  Downloading tenacity-8.4.1-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting future (from PyHive==0.6.5->datasaku==0.0.8)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from Requests==2.30.0->datasaku==0.0.8)\n",
      "  Downloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from Requests==2.30.0->datasaku==0.0.8)\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy==1.4.46->datasaku==0.0.8)\n",
      "  Downloading greenlet-3.0.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl.metadata (3.8 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client==2.72.0->datasaku==0.0.8)\n",
      "  Downloading googleapis_common_protos-1.63.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client==2.72.0->datasaku==0.0.8)\n",
      "  Downloading proto_plus-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client==2.72.0->datasaku==0.0.8)\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client==2.72.0->datasaku==0.0.8)\n",
      "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client==2.72.0->datasaku==0.0.8)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib==0.8.0->datasaku==0.0.8)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=1.19.0->google-api-python-client==2.72.0->datasaku==0.0.8)\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Downloading boto3-1.26.47-py3-none-any.whl (132 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading geopandas-0.12.2-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_python_client-2.72.0-py2.py3-none-any.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading h3-3.7.6-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-5.10.2-py3-none-any.whl (34 kB)\n",
      "Downloading matplotlib-3.6.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading minio-7.1.14-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m77.2/77.2 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-1.5.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (11.4 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading plotly-5.11.0-py2.py3-none-any.whl (15.3 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.23.0-cp37-abi3-manylinux2014_aarch64.whl (303 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.30.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-1.4.46-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.29.165-py3-none-any.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m164.4/164.4 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (136 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m136.6/136.6 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Downloading contourpy-1.2.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m302.1/302.1 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.53.0-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.19.0-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m139.0/139.0 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.30.0-py2.py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m193.7/193.7 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading greenlet-3.0.3-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (657 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m657.7/657.7 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading kiwisolver-1.4.5-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.3.0-cp311-cp311-manylinux_2_28_aarch64.whl (4.3 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyproj-3.6.1-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading s3transfer-0.6.2-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading shapely-2.0.4-cp311-cp311-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.4.1-py3-none-any.whl (27 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m491.3/491.3 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
      "Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.63.1-py2.py3-none-any.whl (229 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m229.2/229.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading proto_plus-1.24.0-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: datasaku, Fiona, PyHive, snowflake\n",
      "  Building editable for datasaku (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for datasaku: filename=datasaku-0.0.8-0.editable-py3-none-any.whl size=3553 sha256=5887b15d11c8568e126b9f252feb5f995c627f144349d8c36dc562e2b043add3\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-n8cysxl0/wheels/7d/b0/f7/718884e02c821debbabd25201967bfb75892ce674d5f4eb086\n",
      "  Building wheel for Fiona (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for Fiona: filename=Fiona-1.8.22-cp311-cp311-linux_aarch64.whl size=3396788 sha256=72129b8bce9feed6a98219260b75e2bd78ef533f8908310eeed8553c0c621258\n",
      "  Stored in directory: /home/datasaku/.cache/pip/wheels/27/cf/75/9047da1d4a19dad7723f3f98b806939c907205097dffae0bb6\n",
      "  Building wheel for PyHive (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyHive: filename=PyHive-0.6.5-py3-none-any.whl size=51557 sha256=b714716bc9d152ad062ea4d0951ca5699e18a9325ce22189c121149e9e154767\n",
      "  Stored in directory: /home/datasaku/.cache/pip/wheels/cc/b2/8d/74115da1b8e1ee44544ec7870783c9fbf1127b66d296f6c4be\n",
      "  Building wheel for snowflake (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for snowflake: filename=snowflake-0.0.3-py3-none-any.whl size=2952 sha256=a9b3e5e8786597bd0dbcbc14d1dd88a91f678bd5f26cf8735653d4e5f86f8ec6\n",
      "  Stored in directory: /home/datasaku/.cache/pip/wheels/66/34/52/0537412bb507a72e4d4e866d8a0d10c59d367fa1cda66b71f3\n",
      "Successfully built datasaku Fiona PyHive snowflake\n",
      "Installing collected packages: snowflake, pytz, h3, fuzzywuzzy, urllib3, uritemplate, tenacity, shapely, pyparsing, pyasn1, protobuf, pillow, oauthlib, munch, kiwisolver, jmespath, importlib-resources, idna, greenlet, future, fonttools, cycler, contourpy, click, charset-normalizer, certifi, cachetools, attrs, SQLAlchemy, rsa, Requests, pyproj, PyHive, pyasn1-modules, proto-plus, plotly, pandas, minio, matplotlib, httplib2, googleapis-common-protos, cligj, click-plugins, botocore, seaborn, s3transfer, requests-oauthlib, google-auth, Fiona, google-auth-oauthlib, google-auth-httplib2, google-api-core, geopandas, boto3, google-api-python-client, datasaku\n",
      "Successfully installed Fiona-1.8.22 PyHive-0.6.5 Requests-2.30.0 SQLAlchemy-1.4.46 attrs-23.2.0 boto3-1.26.47 botocore-1.29.165 cachetools-5.3.3 certifi-2024.6.2 charset-normalizer-3.3.2 click-8.1.7 click-plugins-1.1.1 cligj-0.7.2 contourpy-1.2.1 cycler-0.12.1 datasaku-0.0.8 fonttools-4.53.0 future-1.0.0 fuzzywuzzy-0.18.0 geopandas-0.12.2 google-api-core-2.19.0 google-api-python-client-2.72.0 google-auth-2.30.0 google-auth-httplib2-0.2.0 google-auth-oauthlib-0.8.0 googleapis-common-protos-1.63.1 greenlet-3.0.3 h3-3.7.6 httplib2-0.22.0 idna-3.7 importlib-resources-5.10.2 jmespath-1.0.1 kiwisolver-1.4.5 matplotlib-3.6.2 minio-7.1.14 munch-4.0.0 oauthlib-3.2.2 pandas-1.5.2 pillow-10.3.0 plotly-5.11.0 proto-plus-1.24.0 protobuf-4.23.0 pyasn1-0.6.0 pyasn1-modules-0.4.0 pyparsing-3.1.2 pyproj-3.6.1 pytz-2024.1 requests-oauthlib-2.0.0 rsa-4.9 s3transfer-0.6.2 seaborn-0.12.2 shapely-2.0.4 snowflake-0.0.3 tenacity-8.4.1 uritemplate-4.1.1 urllib3-1.26.15\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -e /data/datasaku/datasaku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-ulid\n",
      "  Downloading python_ulid-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Downloading python_ulid-2.7.0-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: python-ulid\n",
      "Successfully installed python-ulid-2.7.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install python-ulid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init(spark_home='/opt/spark')\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-arm64/'\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/local/bin/python3.11'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/home/datasaku/.venv/bin/python3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20240730'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pendulum\n",
    "pendulum.now().format('YYYYMMDD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasaku import datasaku_spark\n",
    "Spark = datasaku_spark.DatasakuSparkNessieMinioIceberg(\n",
    "    spark_home = '/opt/spark'\n",
    "    , minio_username = 'minio'\n",
    "    , minio_password = 'minio123'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/30 18:39:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = Spark.spark_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/30 18:40:04 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table exist\n"
     ]
    }
   ],
   "source": [
    "if spark._jsparkSession.catalog().tableExists(\"nessie_catalog.datasaku.dagster_test\"):\n",
    "    print(\"table exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'datasaku' in spark.sql(\"SHOW NAMESPACES\").toPandas()['namespace'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------------------+\n",
      "|refType|name|                hash|\n",
      "+-------+----+--------------------+\n",
      "| Branch|main|905c18b3f72bc6a3d...|\n",
      "+-------+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW REFERENCE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+------+\n",
      "|Season|   Team|Salary|Player|\n",
      "+------+-------+------+------+\n",
      "|     1|chicago|   100|kurang|\n",
      "|     2|chicago|   200|kurang|\n",
      "+------+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = spark.createDataFrame(\n",
    "    [\n",
    "        (\"1\", \"chicago\", \"100\", \"kurang\"),  # create your data here, be consistent in the types.\n",
    "        (\"2\", \"chicago\", \"200\", \"kurang\"),\n",
    "    ],\n",
    "    [\"Season\", \"Team\", \"Salary\", \"Player\"]  # add your column names here\n",
    ")\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_column = ['Team','Season','Player']\n",
    "all_column = sdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import farmhash\n",
    "def farmhash_python(string:str):\n",
    "    return farmhash.hash64(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fs\n",
    "from pyspark.sql.types import StringType\n",
    "farmhash_pyspark = fs.udf(lambda x: farmhash_python(x),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+------+----------------+\n",
      "|Season|   Team|Salary|Player|            test|\n",
      "+------+-------+------+------+----------------+\n",
      "|     1|chicago|   100|kurang|chicago-1-kurang|\n",
      "|     2|chicago|   200|kurang|chicago-2-kurang|\n",
      "+------+-------+------+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.withColumn('test', fs.concat_ws('-', *key_column)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+------+--------------------+\n",
      "|Season|   Team|Salary|Player|               __key|\n",
      "+------+-------+------+------+--------------------+\n",
      "|     1|chicago|   100|kurang|14633329190751059452|\n",
      "|     2|chicago|   200|kurang|11836422410755210579|\n",
      "+------+-------+------+------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf = sdf.withColumn('__key', farmhash_pyspark(fs.concat_ws('', *key_column)))\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create unique_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+------+--------------------+--------------------+\n",
      "|Season|   Team|Salary|Player|               __key|     __unique_row_id|\n",
      "+------+-------+------+------+--------------------+--------------------+\n",
      "|     1|chicago|   100|kurang|14633329190751059452|375ff507bc5001fca...|\n",
      "|     2|chicago|   200|kurang|11836422410755210579|8ab1342d6a8fa238e...|\n",
      "+------+-------+------+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf.withColumn('__unique_row_id', fs.md5(fs.concat_ws('', *all_column)))\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create timestamp utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+------+--------------------+--------------------------------+---------------------------+\n",
      "|Season|Team   |Salary|Player|__key               |__unique_row_id                 |__created_timestamp_utc    |\n",
      "+------+-------+------+------+--------------------+--------------------------------+---------------------------+\n",
      "|1     |chicago|100   |kurang|14633329190751059452|375ff507bc5001fca4d66619089e4455|2024-07-31T06:50:46.412475Z|\n",
      "|2     |chicago|200   |kurang|11836422410755210579|8ab1342d6a8fa238ed09af412394aeca|2024-07-31T06:50:46.412475Z|\n",
      "+------+-------+------+------+--------------------+--------------------------------+---------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pendulum\n",
    "sdf = sdf.withColumn('__created_timestamp_utc', fs.lit(pendulum.now('utc').to_iso8601_string()))\n",
    "sdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## is_current_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+------+--------------------+--------------------+-----------------------+-----------+\n",
      "|Season|   Team|Salary|Player|               __key|     __unique_row_id|__created_timestamp_utc|_is_current|\n",
      "+------+-------+------+------+--------------------+--------------------+-----------------------+-----------+\n",
      "|     1|chicago|   100|kurang|14633329190751059452|375ff507bc5001fca...|   2024-07-31T06:50:...|       true|\n",
      "|     2|chicago|   200|kurang|11836422410755210579|8ab1342d6a8fa238e...|   2024-07-31T06:50:...|       true|\n",
      "+------+-------+------+------+--------------------+--------------------+-----------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf = sdf.withColumn('_is_current', fs.lit(True))\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ulid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+------+--------------------+--------------------+-----------------------+-----------+--------------------+\n",
      "|Season|   Team|Salary|Player|               __key|     __unique_row_id|__created_timestamp_utc|_is_current|           _batch_id|\n",
      "+------+-------+------+------+--------------------+--------------------+-----------------------+-----------+--------------------+\n",
      "|     1|chicago|   100|kurang|14633329190751059452|375ff507bc5001fca...|   2024-07-31T06:50:...|       true|01J43RWGS0HZ0CBMC...|\n",
      "|     2|chicago|   200|kurang|11836422410755210579|8ab1342d6a8fa238e...|   2024-07-31T06:50:...|       true|01J43RWGS0HZ0CBMC...|\n",
      "+------+-------+------+------+--------------------+--------------------+-----------------------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ulid import ULID\n",
    "\n",
    "batch_id = str(ULID())\n",
    "\n",
    "sdf = sdf.withColumn('_batch_id', fs.lit(batch_id))\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ulid test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o128.get",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfs\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringType\n\u001b[0;32m----> 7\u001b[0m ulid_pyspark \u001b[38;5;241m=\u001b[39m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mudf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mulid_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mStringType\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m sdf \u001b[38;5;241m=\u001b[39m sdf\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__batch_id\u001b[39m\u001b[38;5;124m'\u001b[39m, ulid_pyspark())\n\u001b[1;32m     10\u001b[0m sdf\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:174\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/functions.py:15767\u001b[0m, in \u001b[0;36mudf\u001b[0;34m(f, returnType, useArrow)\u001b[0m\n\u001b[1;32m  15761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m  15762\u001b[0m         _create_py_udf,\n\u001b[1;32m  15763\u001b[0m         returnType\u001b[38;5;241m=\u001b[39mreturn_type,\n\u001b[1;32m  15764\u001b[0m         useArrow\u001b[38;5;241m=\u001b[39museArrow,\n\u001b[1;32m  15765\u001b[0m     )\n\u001b[1;32m  15766\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m> 15767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_py_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturnType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturnType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43museArrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43museArrow\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/udf.py:127\u001b[0m, in \u001b[0;36m_create_py_udf\u001b[0;34m(f, returnType, useArrow)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m    123\u001b[0m     session \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39m_instantiatedSession\n\u001b[1;32m    124\u001b[0m     is_arrow_enabled \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m session \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.execution.pythonUDF.arrow.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m     )\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     is_arrow_enabled \u001b[38;5;241m=\u001b[39m useArrow\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/conf.py:54\u001b[0m, in \u001b[0;36mRuntimeConfig.get\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkType(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m _NoValue:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o128.get"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "from ulid import ULID\n",
    "def ulid_python():\n",
    "    return str(ULID())\n",
    "\n",
    "import pyspark.sql.functions as fs\n",
    "from pyspark.sql.types import StringType\n",
    "ulid_pyspark = fs.udf(lambda : ulid_python(),StringType())\n",
    "\n",
    "sdf = sdf.withColumn('__batch_id', ulid_pyspark())\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.withColumn('test', fs.concat_ws('-', *key_column)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pendulum\n",
    "def pendulum_now_python(string:str):\n",
    "    return pendulum.now().to_date_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fs\n",
    "from pyspark.sql.types import StringType\n",
    "pendulum_now_pyspark = fs.udf(lambda x: pendulum_now_python(x),StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/13 21:08:46 WARN TaskSetManager: Lost task 0.0 in stage 14.0 (TID 13) (10.244.0.24 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'pendulum'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/07/13 21:08:47 ERROR TaskSetManager: Task 0 in stage 14.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'pendulum'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTimestamp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpendulum_now_pyspark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTeam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'pendulum'\n"
     ]
    }
   ],
   "source": [
    "sdf.withColumn('Timestamp', pendulum_now_pyspark(fs.col('Team'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/16 20:52:50 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------+-------+------+------+--------------------+-----------------------+------------+--------------+-------------------+\n",
      "|Unnamed: 0.1|Unnamed: 0|Season|   Team|Salary|Player|     __unique_row_id|__created_timestamp_utc|__is_current|__logical_date|test_staging_column|\n",
      "+------------+----------+------+-------+------+------+--------------------+-----------------------+------------+--------------+-------------------+\n",
      "|           0|         0|     1|chicago|   100|kurang|375ff507bc5001fca...|   2024-07-07T12:54:...|        true|      20240707|test_staging_column|\n",
      "|           0|         0|     1|chicago|   100|kurang|375ff507bc5001fca...|   2024-07-14T17:38:...|        true|      20240702|test_staging_column|\n",
      "|           0|         0|     1|chicago|   100|kurang|375ff507bc5001fca...|   2024-07-13T11:03:...|        true|      20240708|test_staging_column|\n",
      "|           0|         0|     1|chicago|   100|kurang|375ff507bc5001fca...|   2024-07-16T20:47:...|        true|      20240707|test_staging_column|\n",
      "|           0|         0|     1|chicago|   100|kurang|375ff507bc5001fca...|   2024-07-13T21:36:...|        true|      20240706|test_staging_column|\n",
      "|           1|         1|     2|chicago|   200|kurang|8ab1342d6a8fa238e...|   2024-07-07T12:54:...|        true|      20240707|test_staging_column|\n",
      "|           1|         1|     2|chicago|   200|kurang|8ab1342d6a8fa238e...|   2024-07-16T20:47:...|        true|      20240707|test_staging_column|\n",
      "|           1|         1|     2|chicago|   200|kurang|8ab1342d6a8fa238e...|   2024-07-14T17:38:...|        true|      20240702|test_staging_column|\n",
      "|           1|         1|     2|chicago|   200|kurang|8ab1342d6a8fa238e...|   2024-07-13T11:03:...|        true|      20240708|test_staging_column|\n",
      "|           1|         1|     2|chicago|   200|kurang|8ab1342d6a8fa238e...|   2024-07-13T21:36:...|        true|      20240706|test_staging_column|\n",
      "+------------+----------+------+-------+------+------+--------------------+-----------------------+------------+--------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/16 21:18:16 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "24/07/16 21:18:17 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:981)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('iceberg').load('nessie_catalog.datasaku.dagster_test').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/31 06:51:47 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/31 07:12:58 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "24/07/31 07:12:59 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:981)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"drop table nessie_catalog.datasaku.dagster_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
